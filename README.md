![CLEAR-AI Workshop Banner](workshop.jpg)

# CLEAR-AI Workshop: Collaborative Methods and Tools for Engineering and Evaluating Transparency in AI [ECAI 2025]

## Workshop Schedule

### Morning Session

#### 09:30‚Äì10:30 | Welcome & Introduction
**Presentation:** "Enhancing Active Learning efficiency with a Recommendation System based on Annotator Accuracy, Mood, and Fatigue"  
*Diana Mort√°gua, Luis Macedo and F. Am√≠lcar Cardoso*

---

#### 10:30‚Äì11:00 | ‚òï Coffee Break

---

#### 11:00‚Äì12:30 | Session 1: Presentations

**"A Visual Reader's Eye for Image-Generation AI"**  
*Randi Cecchine and Martha Larson*

**"Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation"**  
*Teodor Chiaburu, Vipin Singh, Frank Hau√üer and Felix Biessmann*

**"Towards Transparent and Interpretable Credit Risk Models: Classifier Selection Insights from Australia, Germany, and Taiwan"**  
*Krzysztof Lorenz, Piotr Gutowski, Anna Drab-Kurowska, Agnieszka Budziewicz-Gu≈∫lecka, Ewelina Gutowska, Magdalena Majchrzak, Tymoteusz Miller, Irmina Durlik and Ewelina Kostecka*

---

#### 12:30‚Äì14:00 | üçΩÔ∏è Lunch Break

---

### Afternoon Session

#### 14:00‚Äì15:30 | Session 2: Presentations

**"Information Flow Modeling for Transparent and Accountable AI Act Assessment"**  
*Mattias Br√§nnstr√∂m, Themis Xanthopoulou and Lili Jiang*

**"The Machinery of Government: Bureaucracy, Automation and Institutional Black-Boxing"**  
*Diletta Huyskes*

**"Making Privacy Risks Transparent: Causal Analysis of Generalization and Membership Inference Attack in Differentially Private SGD"**  
*Zhou Zhou and Lili Jiang*

---

#### 15:30‚Äì16:00 | ‚òï Coffee Break

---

#### 16:00‚Äì17:00 | Roundtable Discussion
Interactive discussion with presenters and organizers

---

#### 17:00‚Äì17:15 | Closing Remarks

---


## Accepted Papers

The following papers have been accepted for presentation at the CLEAR-AI Workshop:

**"Enhancing Active Learning efficiency with a Recommendation System based on Annotator Accuracy, Mood, and Fatigue"**  
*Diana Mort√°gua, Luis Macedo and F. Am√≠lcar Cardoso*

**"A Visual Reader's Eye for Image-Generation AI"**  
*Randi Cecchine and Martha Larson*

**"Uncertainty-Guided Expert-AI Collaboration for Efficient Soil Horizon Annotation"**  
*Teodor Chiaburu, Vipin Singh, Frank Hau√üer and Felix Biessmann*

**"Towards Transparent and Interpretable Credit Risk Models: Classifier Selection Insights from Australia, Germany, and Taiwan"**  
*Krzysztof Lorenz, Piotr Gutowski, Anna Drab-Kurowska, Agnieszka Budziewicz-Gu≈∫lecka, Ewelina Gutowska, Magdalena Majchrzak, Tymoteusz Miller, Irmina Durlik and Ewelina Kostecka*

**"Information Flow Modeling for Transparent and Accountable AI Act Assessment"**  
*Mattias Br√§nnstr√∂m, Themis Xanthopoulou and Lili Jiang*

**"The Machinery of Government: Bureaucracy, Automation and Institutional Black-Boxing"**  
*Diletta Huyskes*

**"Making Privacy Risks Transparent: Causal Analysis of Generalization and Membership Inference Attack in Differentially Private SGD"**  
*Zhou Zhou and Lili Jiang*


## About the Workshop

The CLEAR-AI Workshop brings together researchers, practitioners, and stakeholders to address transparency in AI systems through collaborative and participatory approaches. Below you'll find more details about the workshop's focus, submission guidelines, and how to participate.

## Background
As AI systems become integral to critical decision-making processes, ensuring their transparency, fairness, and accountability is more essential than ever. Therefore, it is not surprising that the AI Act, the world's first attempt to systematically regulate artificial intelligence, places significant emphasis on this need. In particular, the Act establishes transparency as a fundamental principle, aiming to safeguard user rights and foster trust and reliability in new technologies. In this context, transparency assumes a salient value as a prerequisite for ensuring the fair and responsible development and deployment of AI.

Achieving these objectives necessitates not only the establishment of robust technical frameworks but also the active engagement of a diverse range of stakeholders to ensure that the development of AI is aligned with societal values. In this regard, interdisciplinary research and implementation initiatives have been identified as crucial to facilitate progress in this area.

Following from this, the workshop focuses on advancing the design, monitoring, and evaluation of transparent AI systems. By combining participatory approaches that amplify stakeholder voices with formal methodologies that ensure rigour and reproducibility, the workshop aims to advance the state of the art in trustworthy AI, bridging technical and social perspectives.

The CLEAR-AI Workshop addresses the critical need for interdisciplinary collaboration to advance methods, tools, and evaluation frameworks that ensure transparency, fairness, and trustworthiness in AI systems. In addition, specific focus is put on tackling the challenges emerging from the participatory setting. Submissions should emphasize the integration of technical rigour with societal relevance to address the challenges of building trustworthy AI systems. The CLEAR-AI Workshop aligns with ECAI 2025's mission of advancing sustainable AI by promoting transparency, participatory approaches, and rigorous methods that ensure accountability and societal alignment.



## Problem Description
Transparency takes up a central instrumental role in addressing and understanding a wide range of problems from legal to ethical in AI development and deployment. Yet, transparency is often a vague and hard to pin down notion from a objective perspective and is defined differently on different levels in the understanding of AI systems.

In CLEAR we want to approach the problem from the direction of *stakeholder needs* on transparency. By understanding the needs for information we can work to concretely meet these demands. 

The needs on transparency can come from many different sources, such as legal understanding and effective user agency, as such any workable method for this will have to address the participatory aspect of eliciting these needs from the stakeholders.

## Topics of Interest
The workshop welcomes contributions on topics including, but not limited to:
- Innovative methodologies for designing transparent AI systems for meeting stakeholder needs
- Tools, metrics, and techniques for monitoring and ensuring transparency in AI
- Transparency risk assessments
- Participatory design and co-creation methodological approaches involving diverse stakeholders
- Formal frameworks for evaluating transparency
- Case studies and best practices for transparent AI in real-world applications
- Knowledge elicitation methods during participatory settings for AI transparency
- Conceptual translation among stakeholders from diverse backgrounds
- Legal transparency requirements

Submissions should emphasize the integration of technical rigour with societal relevance to address the challenges of building trustworthy AI systems. 

Perspectives on the above issues from under-represented countries are particularly welcome.

## Challenges and Workshop perspective

### Challenge 1. Reframing Transparency as a Relation, Not a Static Property

Transparency is too often treated as something systems have rather than something they provide in relation to those who use, audit, or are affected by them.
This theme challenges us to rethink transparency as an active relation between information and its audience, asking what kinds of transparency actually reach and serve their intended stakeholders. Taking a step back this theme pushes us to reflect about the stakeholders behind the system design and their agency through the system.


### Challenge 2. Connecting Methods to Meaning and Purpose

Technical tools and conceptual models for transparency abound, but they often operate without a clear sense of for whom and for what they exist.
This theme explores how metrics, explanations, and frameworks can be oriented toward concrete purposes‚Äîwhether legal accountability, informed oversight, or effective user understanding‚Äîrather than remaining abstract or self-referential.
                                                                                                                                                                                                     ### Challenge 3. Moving Toward Stakeholder-Centered Transparency

The ultimate challenge remains to make transparency usable, relevant, and responsive.
This theme looks at how today‚Äôs methods can evolve‚Äîthrough participatory, methodological, or institutional innovation‚Äîtoward transparency that genuinely empowers those who depend on it.

## Target Audience
The CLEAR-AI Workshop targets an interdisciplinary audience, including:
- Researchers and practitioners in AI, machine learning, and data science
- Experts in human-computer interaction and participatory design
- Ethicists, policymakers, and legal professionals focusing on AI governance
- Developers and engineers working on tools and frameworks for AI transparency
- Stakeholders from academia, industry, and civil society interested in AI transparency



## Submission guidelines 
Authors are invited to submit full papers of 5,000 to 8,000 words following the ACM guidlines [https://www.acm.org/publications/authors/submissions](https://www.acm.org/publications/authors/submissions) . Submissions will be reviewed by at least two reviewers in a double blind review process. Paper, including transfer papers from the ECAI main track, should be submitted through our Easychair [link](https://easychair.org/conferences?conf=clearwsecai2025). At least one author of each accepted paper is required to register for the workshop and for ECAI. Informal proceedings will be distributed to ECAI 2025 registrants in electronic form.

## üìö Special Issue: ACM Journal on Responsible Computing

### Extended Publication Opportunity

Following the workshop, we are pleased to announce a **special section** in the [ACM Journal on Responsible Computing (JRC)](https://dl.acm.org/journal/jrc) dedicated to transparency in AI systems. This special section will be part of a regular issue of the JRC, with additional submissions from workshop participants also welcomed.

### üéØ Submission Requirements

**Post-Workshop Revision:**
- Connect your original work to the three main challenges addressed in the workshop. Additional submissions by workshop participants will also be considered as long as they connect to the workshop themes.
- Incorporate feedback received during your presentation
- Expand and deepen your analysis with additional perspectives


### üìÖ Timeline

| **Submission Deadline** | **31 January 2026** |


### ‚úÖ Publication Details

- **Open Access:** Policy to be confirmed [TBC]
- **Repository:** Details to be confirmed [TBC]
- **Review Process:** 
- Workshop organizers will serve as guest editors
- Reviewers will include a mix of paper authors, workshop participants, original reviewers, and external reviewers

### üí° Contact person

**Francien Dechesne**  
üìß [f.dechesne@law.leidenuniv.nl](mailto:f.dechesne@law.leidenuniv.nl)



## Important Dates
- Open for submissions: Tuesday, 15 April 2025
- 1st Paper submission cut-off date: Sunday, 15 June 2025
- Notification date for 1st round of submission: Tuesday, 15 July 2025
- 2nd Paper submission cut-off date (new papers and manual transfer from main conference): Friday,  15 August 2025
- Notification date for 2nd round of submission: Wednesday, 20 August 2025
- Final workshop schedule published: Wednesday, 22 October 2025

## Venue
- EUROPEAN CONFERENCE ON ARTIFICIAL INTELLIGENCE (ECAI 2025), Bologna, Italy
- Registration and information: [https://ecai2025.org/](https://ecai2025.org/)




## Organising Committee
- **Themis Dimitra Xanthopoulou**, Department of Computing Science, Ume√• University
- **Rachele Carli**, Department of Computing Science, Ume√• University
- **Andreas Br√§nnstr√∂m**, Department of Computing Science, Ume√• University
- **Francien Dechesne**, eLaw Center for Law and Digital Technologies, Leiden University
- **Chiara Gallese**, Department of Law, University of Turin
- **Mattias Br√§nnstr√∂m**, Founder of Compley AB

## Advisory Board
- Virginia Dignum, AI Policy Lab
- Juan Carlos Nieves, Ume√• University
- Neil Yorke-Smith, TU Delft
- Tim Miller, University of Queensland

## Program Committee
- Tony Ribeiro, Laboratoire des Sciences du Num√©rique de Nantes
- Agate Balayn, TU Delft
- Daniel Kostic, Leiden University
- Giuseppe Primiero, University of Milan
- Jayati Deshmukh, University of Southampton
- Imane Hmiddou, ALLAI
- Amro Najjar, Luxembourg Institute of Science and Technology

## Contact
Please direct any questions regarding the workshop to:
- Themis Dimitra Xanthopoulou, Ume√• University, Sweden, [themis.xanthopoulou@umu.se](mailto:themis.xanthopoulou@umu.se)
